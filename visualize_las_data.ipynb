{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20781321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import laspy\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a3930d",
   "metadata": {},
   "source": [
    "## Load LAS Files from data_red folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5713de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data directory\n",
    "data_dir = Path('data_red')\n",
    "\n",
    "# List all .las files\n",
    "las_files = list(data_dir.glob('*.las'))\n",
    "print(f\"Found {len(las_files)} LAS files:\")\n",
    "for f in las_files:\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cb964a",
   "metadata": {},
   "source": [
    "## Load and Inspect a Single LAS File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db761235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the first LAS file\n",
    "las_file_path = las_files[0]\n",
    "print(f\"Loading: {las_file_path}\")\n",
    "\n",
    "# Read the LAS file\n",
    "las_data = laspy.read(las_file_path)\n",
    "\n",
    "# Print basic information\n",
    "print(f\"\\nNumber of points: {len(las_data.points)}\")\n",
    "print(f\"Point format: {las_data.point_format}\")\n",
    "print(f\"\\nAvailable dimensions:\")\n",
    "for dim in las_data.point_format.dimension_names:\n",
    "    print(f\"  - {dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74d77d0",
   "metadata": {},
   "source": [
    "## Extract Point Cloud Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f79bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates\n",
    "x = las_data.x\n",
    "y = las_data.y\n",
    "z = las_data.z\n",
    "\n",
    "print(f\"X range: [{x.min():.2f}, {x.max():.2f}]\")\n",
    "print(f\"Y range: [{y.min():.2f}, {y.max():.2f}]\")\n",
    "print(f\"Z range: [{z.min():.2f}, {z.max():.2f}]\")\n",
    "\n",
    "# Check for classification (labels)\n",
    "if hasattr(las_data, 'classification'):\n",
    "    classification = las_data.classification\n",
    "    unique_classes = np.unique(classification)\n",
    "    print(f\"\\nUnique classification labels: {unique_classes}\")\n",
    "    print(f\"Number of unique classes: {len(unique_classes)}\")\n",
    "    for cls in unique_classes:\n",
    "        count = np.sum(classification == cls)\n",
    "        print(f\"  Class {cls}: {count} points ({count/len(classification)*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"\\nNo classification information found\")\n",
    "\n",
    "# Check for RGB colors\n",
    "has_rgb = False\n",
    "if hasattr(las_data, 'red') and hasattr(las_data, 'green') and hasattr(las_data, 'blue'):\n",
    "    has_rgb = True\n",
    "    red = las_data.red\n",
    "    green = las_data.green\n",
    "    blue = las_data.blue\n",
    "    print(f\"\\nRGB information available\")\n",
    "else:\n",
    "    print(f\"\\nNo RGB information available\")\n",
    "\n",
    "# Check for intensity\n",
    "if hasattr(las_data, 'intensity'):\n",
    "    intensity = las_data.intensity\n",
    "    print(f\"Intensity range: [{intensity.min()}, {intensity.max()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93083da4",
   "metadata": {},
   "source": [
    "## 3D Visualization - Full Point Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3f50b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample for faster visualization (take every N-th point)\n",
    "subsample_factor = max(1, len(x) // 50000)  # Limit to ~50k points\n",
    "x_sub = x[::subsample_factor]\n",
    "y_sub = y[::subsample_factor]\n",
    "z_sub = z[::subsample_factor]\n",
    "\n",
    "print(f\"Visualizing {len(x_sub)} points (subsampled by factor {subsample_factor})\")\n",
    "\n",
    "# Create 3D plot\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot points colored by height (z-coordinate)\n",
    "scatter = ax.scatter(x_sub, y_sub, z_sub, c=z_sub, cmap='viridis', s=1, alpha=0.6)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z (Height)')\n",
    "ax.set_title(f'Point Cloud: {las_file_path.name}')\n",
    "plt.colorbar(scatter, label='Height', shrink=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5798e928",
   "metadata": {},
   "source": [
    "## 3D Visualization - Colored by Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60da12d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(las_data, 'classification'):\n",
    "    classification_sub = classification[::subsample_factor]\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    scatter = ax.scatter(x_sub, y_sub, z_sub, c=classification_sub, \n",
    "                        cmap='tab10', s=1, alpha=0.6)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z (Height)')\n",
    "    ax.set_title(f'Point Cloud Colored by Classification: {las_file_path.name}')\n",
    "    plt.colorbar(scatter, label='Class', shrink=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Classification not available for this file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf4f694",
   "metadata": {},
   "source": [
    "## Top-Down View (2D Projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfbb0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(x_sub, y_sub, c=z_sub, cmap='viridis', s=0.5, alpha=0.6)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title(f'Top-Down View (colored by height): {las_file_path.name}')\n",
    "plt.colorbar(label='Height (Z)')\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1716fd",
   "metadata": {},
   "source": [
    "## Height Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d426253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.hist(z, bins=100, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Height (Z)')\n",
    "plt.ylabel('Number of Points')\n",
    "plt.title(f'Height Distribution: {las_file_path.name}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1b4aee",
   "metadata": {},
   "source": [
    "## Convert to Numpy Array for Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465c35a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a numpy array with XYZ coordinates\n",
    "points = np.vstack((x, y, z)).T\n",
    "print(f\"Point cloud shape: {points.shape}\")\n",
    "print(f\"Data type: {points.dtype}\")\n",
    "\n",
    "# Normalize coordinates (important for deep learning)\n",
    "points_normalized = points - points.mean(axis=0)\n",
    "max_distance = np.max(np.linalg.norm(points_normalized, axis=1))\n",
    "points_normalized = points_normalized / max_distance\n",
    "\n",
    "print(f\"\\nNormalized point cloud shape: {points_normalized.shape}\")\n",
    "print(f\"Normalized range: [{points_normalized.min():.3f}, {points_normalized.max():.3f}]\")\n",
    "\n",
    "# If we have additional features, we can concatenate them\n",
    "features_list = [points]\n",
    "\n",
    "if hasattr(las_data, 'intensity'):\n",
    "    # Normalize intensity\n",
    "    intensity_norm = (intensity - intensity.min()) / (intensity.max() - intensity.min() + 1e-8)\n",
    "    features_list.append(intensity_norm.reshape(-1, 1))\n",
    "    print(\"Added intensity feature\")\n",
    "\n",
    "if has_rgb:\n",
    "    # Normalize RGB\n",
    "    rgb = np.vstack((red, green, blue)).T\n",
    "    rgb_norm = rgb / 65535.0  # LAS RGB values are typically 16-bit\n",
    "    features_list.append(rgb_norm)\n",
    "    print(\"Added RGB features\")\n",
    "\n",
    "# Concatenate all features\n",
    "point_features = np.hstack(features_list)\n",
    "print(f\"\\nFinal feature array shape: {point_features.shape}\")\n",
    "print(f\"Features per point: {point_features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cc290a",
   "metadata": {},
   "source": [
    "## Load and Compare Multiple Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b491ce87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all files and show basic statistics\n",
    "print(\"Summary of all LAS files in data_red:\\n\")\n",
    "print(f\"{'Filename':<25} {'Points':<12} {'Classes':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for las_file in las_files:\n",
    "    las = laspy.read(las_file)\n",
    "    n_points = len(las.points)\n",
    "    \n",
    "    if hasattr(las, 'classification'):\n",
    "        n_classes = len(np.unique(las.classification))\n",
    "    else:\n",
    "        n_classes = \"N/A\"\n",
    "    \n",
    "    print(f\"{las_file.name:<25} {n_points:<12} {str(n_classes):<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b42670",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. Loading .las files using laspy\n",
    "2. Extracting XYZ coordinates and other attributes (classification, intensity, RGB)\n",
    "3. 3D visualization of point clouds\n",
    "4. Data preprocessing for deep learning (normalization)\n",
    "5. Creating feature arrays combining multiple attributes\n",
    "\n",
    "Next steps:\n",
    "- Use these preprocessed point clouds for training PointNet models\n",
    "- Implement data augmentation techniques\n",
    "- Create train/test splits"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
